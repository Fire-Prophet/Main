#!/usr/bin/env python3
"""
Data Analyzer Module
ë°ì´í„° ë¶„ì„, í†µê³„, ì¸ì‚¬ì´íŠ¸ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Union, Tuple
import logging
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class DataAnalyzer:
    """ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.logger = logging.getLogger(__name__)
        self.analysis_results = {}
    
    def descriptive_statistics(self, df: pd.DataFrame, 
                             columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ê¸°ìˆ í†µê³„ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            columns: ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
            
        Returns:
            ê¸°ìˆ í†µê³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        results = {}
        
        for col in columns:
            if col not in df.columns:
                continue
            
            series = df[col].dropna()
            
            results[col] = {
                'count': len(series),
                'mean': series.mean(),
                'median': series.median(),
                'mode': series.mode().iloc[0] if len(series.mode()) > 0 else None,
                'std': series.std(),
                'var': series.var(),
                'min': series.min(),
                'max': series.max(),
                'range': series.max() - series.min(),
                'q1': series.quantile(0.25),
                'q3': series.quantile(0.75),
                'iqr': series.quantile(0.75) - series.quantile(0.25),
                'skewness': series.skew(),
                'kurtosis': series.kurtosis(),
                'missing_count': df[col].isnull().sum(),
                'missing_percentage': (df[col].isnull().sum() / len(df)) * 100
            }
        
        self.analysis_results['descriptive_stats'] = results
        self.logger.info(f"ê¸°ìˆ í†µê³„ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ì»¬ëŸ¼")
        return results
    
    def correlation_analysis(self, df: pd.DataFrame,
                           method: str = 'pearson',
                           threshold: float = 0.5) -> Dict[str, Any]:
        """
        ìƒê´€ê´€ê³„ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            method: ìƒê´€ê³„ìˆ˜ ë°©ë²• ('pearson', 'spearman', 'kendall')
            threshold: ê°•í•œ ìƒê´€ê´€ê³„ ì„ê³„ê°’
            
        Returns:
            ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼
        """
        numeric_df = df.select_dtypes(include=[np.number])
        
        if numeric_df.empty:
            self.logger.warning("ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚°
        corr_matrix = numeric_df.corr(method=method)
        
        # ê°•í•œ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) >= threshold:
                    strong_correlations.append({
                        'variable1': corr_matrix.columns[i],
                        'variable2': corr_matrix.columns[j],
                        'correlation': corr_value,
                        'strength': 'strong' if abs(corr_value) >= 0.7 else 'moderate'
                    })
        
        results = {
            'correlation_matrix': corr_matrix.to_dict(),
            'strong_correlations': strong_correlations,
            'method': method,
            'threshold': threshold
        }
        
        self.analysis_results['correlation'] = results
        self.logger.info(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ: {len(strong_correlations)}ê°œ ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬")
        return results
    
    def distribution_analysis(self, df: pd.DataFrame,
                            columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ë¶„í¬ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            columns: ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë¶„í¬ ë¶„ì„ ê²°ê³¼
        """
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        results = {}
        
        for col in columns:
            if col not in df.columns:
                continue
            
            series = df[col].dropna()
            
            # ì •ê·œì„± ê²€ì • (Shapiro-Wilk test)
            if len(series) <= 5000:  # í‘œë³¸ í¬ê¸° ì œí•œ
                try:
                    shapiro_stat, shapiro_p = stats.shapiro(series)
                    is_normal = shapiro_p > 0.05
                except:
                    shapiro_stat, shapiro_p = None, None
                    is_normal = False
            else:
                shapiro_stat, shapiro_p = None, None
                is_normal = False
            
            # íˆìŠ¤í† ê·¸ë¨ ë¶„ì„
            hist, bins = np.histogram(series, bins=30)
            
            results[col] = {
                'shapiro_statistic': shapiro_stat,
                'shapiro_p_value': shapiro_p,
                'is_normal_distribution': is_normal,
                'unique_values': series.nunique(),
                'histogram_data': {
                    'counts': hist.tolist(),
                    'bins': bins.tolist()
                }
            }
        
        self.analysis_results['distribution'] = results
        self.logger.info(f"ë¶„í¬ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ì»¬ëŸ¼")
        return results
    
    def outlier_analysis(self, df: pd.DataFrame,
                        columns: Optional[List[str]] = None,
                        methods: List[str] = ['iqr', 'zscore']) -> Dict[str, Any]:
        """
        ì´ìƒì¹˜ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            columns: ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            methods: ì´ìƒì¹˜ íƒì§€ ë°©ë²•
            
        Returns:
            ì´ìƒì¹˜ ë¶„ì„ ê²°ê³¼
        """
        if columns is None:
            columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        results = {}
        
        for col in columns:
            if col not in df.columns:
                continue
            
            series = df[col].dropna()
            col_results = {}
            
            # IQR ë°©ë²•
            if 'iqr' in methods:
                Q1 = series.quantile(0.25)
                Q3 = series.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers_iqr = series[(series < lower_bound) | (series > upper_bound)]
                
                col_results['iqr'] = {
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound,
                    'outlier_count': len(outliers_iqr),
                    'outlier_percentage': (len(outliers_iqr) / len(series)) * 100,
                    'outlier_values': outliers_iqr.tolist()
                }
            
            # Z-Score ë°©ë²•
            if 'zscore' in methods:
                z_scores = np.abs(stats.zscore(series))
                outliers_zscore = series[z_scores > 3]
                
                col_results['zscore'] = {
                    'threshold': 3,
                    'outlier_count': len(outliers_zscore),
                    'outlier_percentage': (len(outliers_zscore) / len(series)) * 100,
                    'outlier_values': outliers_zscore.tolist()
                }
            
            results[col] = col_results
        
        self.analysis_results['outliers'] = results
        self.logger.info(f"ì´ìƒì¹˜ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ì»¬ëŸ¼")
        return results
    
    def time_series_analysis(self, df: pd.DataFrame,
                           datetime_column: str,
                           value_columns: List[str]) -> Dict[str, Any]:
        """
        ì‹œê³„ì—´ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            datetime_column: ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼
            value_columns: ë¶„ì„í•  ê°’ ì»¬ëŸ¼ë“¤
            
        Returns:
            ì‹œê³„ì—´ ë¶„ì„ ê²°ê³¼
        """
        if datetime_column not in df.columns:
            self.logger.error(f"ë‚ ì§œ ì»¬ëŸ¼ '{datetime_column}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
        ts_df = df.copy()
        ts_df[datetime_column] = pd.to_datetime(ts_df[datetime_column])
        ts_df = ts_df.sort_values(datetime_column)
        
        results = {
            'period': {
                'start_date': ts_df[datetime_column].min(),
                'end_date': ts_df[datetime_column].max(),
                'total_days': (ts_df[datetime_column].max() - ts_df[datetime_column].min()).days
            },
            'trends': {},
            'seasonality': {},
            'statistics': {}
        }
        
        for col in value_columns:
            if col not in ts_df.columns:
                continue
            
            series = ts_df.set_index(datetime_column)[col].dropna()
            
            # ì¶”ì„¸ ë¶„ì„
            if len(series) > 1:
                x = np.arange(len(series))
                slope, intercept, r_value, p_value, std_err = stats.linregr(x, series.values)
                
                results['trends'][col] = {
                    'slope': slope,
                    'r_squared': r_value**2,
                    'p_value': p_value,
                    'trend_direction': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable'
                }
            
            # ì›”ë³„/ì£¼ë³„ íŒ¨í„´ ë¶„ì„
            monthly_stats = series.groupby(series.index.month).agg(['mean', 'std']).to_dict()
            weekly_stats = series.groupby(series.index.dayofweek).agg(['mean', 'std']).to_dict()
            
            results['seasonality'][col] = {
                'monthly_pattern': monthly_stats,
                'weekly_pattern': weekly_stats
            }
            
            # ê¸°ë³¸ í†µê³„
            results['statistics'][col] = {
                'mean': series.mean(),
                'std': series.std(),
                'min': series.min(),
                'max': series.max(),
                'volatility': series.std() / series.mean() if series.mean() != 0 else 0
            }
        
        self.analysis_results['time_series'] = results
        self.logger.info(f"ì‹œê³„ì—´ ë¶„ì„ ì™„ë£Œ: {len(value_columns)}ê°œ ë³€ìˆ˜")
        return results
    
    def categorical_analysis(self, df: pd.DataFrame,
                           columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
        
        Args:
            df: ì…ë ¥ DataFrame
            columns: ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë²”ì£¼í˜• ë¶„ì„ ê²°ê³¼
        """
        if columns is None:
            columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        results = {}
        
        for col in columns:
            if col not in df.columns:
                continue
            
            series = df[col].dropna()
            value_counts = series.value_counts()
            
            results[col] = {
                'unique_count': series.nunique(),
                'most_frequent': value_counts.index[0] if len(value_counts) > 0 else None,
                'most_frequent_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,
                'least_frequent': value_counts.index[-1] if len(value_counts) > 0 else None,
                'least_frequent_count': value_counts.iloc[-1] if len(value_counts) > 0 else 0,
                'frequency_distribution': value_counts.to_dict(),
                'missing_count': df[col].isnull().sum(),
                'missing_percentage': (df[col].isnull().sum() / len(df)) * 100,
                'entropy': stats.entropy(value_counts.values)  # ì •ë³´ ì—”íŠ¸ë¡œí”¼
            }
        
        self.analysis_results['categorical'] = results
        self.logger.info(f"ë²”ì£¼í˜• ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ì»¬ëŸ¼")
        return results
    
    def hypothesis_testing(self, df: pd.DataFrame,
                          group_column: str,
                          value_column: str,
                          test_type: str = 'auto') -> Dict[str, Any]:
        """
        ê°€ì„¤ ê²€ì •
        
        Args:
            df: ì…ë ¥ DataFrame
            group_column: ê·¸ë£¹ ë³€ìˆ˜
            value_column: ê°’ ë³€ìˆ˜
            test_type: ê²€ì • ë°©ë²• ('auto', 'ttest', 'anova', 'kruskal')
            
        Returns:
            ê°€ì„¤ ê²€ì • ê²°ê³¼
        """
        if group_column not in df.columns or value_column not in df.columns:
            self.logger.error("ì§€ì •ëœ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        # ê·¸ë£¹ë³„ ë°ì´í„° ë¶„ë¦¬
        groups = df.groupby(group_column)[value_column].apply(list).to_dict()
        group_names = list(groups.keys())
        group_data = [np.array(groups[name]) for name in group_names]
        
        # ê·¸ë£¹ì´ 2ê°œì¸ ê²½ìš°
        if len(group_data) == 2:
            if test_type in ['auto', 'ttest']:
                # t-test
                statistic, p_value = stats.ttest_ind(group_data[0], group_data[1])
                test_name = 'Independent t-test'
            else:
                # Mann-Whitney U test
                statistic, p_value = stats.mannwhitneyu(group_data[0], group_data[1])
                test_name = 'Mann-Whitney U test'
        
        # ê·¸ë£¹ì´ 3ê°œ ì´ìƒì¸ ê²½ìš°
        elif len(group_data) > 2:
            if test_type in ['auto', 'anova']:
                # ANOVA
                statistic, p_value = stats.f_oneway(*group_data)
                test_name = 'One-way ANOVA'
            else:
                # Kruskal-Wallis test
                statistic, p_value = stats.kruskal(*group_data)
                test_name = 'Kruskal-Wallis test'
        else:
            self.logger.error("ë¶„ì„í•  ê·¸ë£¹ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        # íš¨ê³¼ í¬ê¸° ê³„ì‚° (Cohen's d for 2 groups)
        effect_size = None
        if len(group_data) == 2:
            pooled_std = np.sqrt(((len(group_data[0])-1)*np.var(group_data[0]) + 
                                (len(group_data[1])-1)*np.var(group_data[1])) / 
                               (len(group_data[0])+len(group_data[1])-2))
            if pooled_std != 0:
                effect_size = (np.mean(group_data[0]) - np.mean(group_data[1])) / pooled_std
        
        results = {
            'test_name': test_name,
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'effect_size': effect_size,
            'groups': {name: {
                'count': len(groups[name]),
                'mean': np.mean(groups[name]),
                'std': np.std(groups[name])
            } for name in group_names}
        }
        
        self.analysis_results['hypothesis_test'] = results
        self.logger.info(f"ê°€ì„¤ ê²€ì • ì™„ë£Œ: {test_name}")
        return results
    
    def generate_insights(self, df: pd.DataFrame) -> List[str]:
        """
        ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±
        
        Args:
            df: ì…ë ¥ DataFrame
            
        Returns:
            ì¸ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        insights = []
        
        # ê¸°ë³¸ ì •ë³´
        insights.append(f"ë°ì´í„°ì…‹ í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # ê²°ì¸¡ê°’ ì •ë³´
        missing_pct = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
        if missing_pct > 10:
            insights.append(f"âš ï¸ ì „ì²´ ë°ì´í„°ì˜ {missing_pct:.1f}%ê°€ ê²°ì¸¡ê°’ì…ë‹ˆë‹¤.")
        
        # ì¤‘ë³µê°’ ì •ë³´
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            insights.append(f"âš ï¸ {duplicates:,}ê°œì˜ ì¤‘ë³µ í–‰ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì¸ì‚¬ì´íŠ¸
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            insights.append(f"ğŸ“Š {len(numeric_cols)}ê°œì˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.")
            
            # ìƒê´€ê´€ê³„ê°€ ê°•í•œ ë³€ìˆ˜ ìŒ
            if 'correlation' in self.analysis_results:
                strong_corr = self.analysis_results['correlation']['strong_correlations']
                if strong_corr:
                    insights.append(f"ğŸ”— {len(strong_corr)}ê°œì˜ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì‚¬ì´íŠ¸
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(cat_cols) > 0:
            insights.append(f"ğŸ“ {len(cat_cols)}ê°œì˜ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤.")
            
            high_cardinality = []
            for col in cat_cols:
                if df[col].nunique() > df.shape[0] * 0.8:
                    high_cardinality.append(col)
            
            if high_cardinality:
                insights.append(f"âš ï¸ ë†’ì€ ì¹´ë””ë„ë¦¬í‹°ë¥¼ ê°€ì§„ ë³€ìˆ˜: {', '.join(high_cardinality)}")
        
        # ì´ìƒì¹˜ ì •ë³´
        if 'outliers' in self.analysis_results:
            outlier_cols = []
            for col, methods in self.analysis_results['outliers'].items():
                for method, result in methods.items():
                    if result['outlier_percentage'] > 5:
                        outlier_cols.append(col)
                        break
            
            if outlier_cols:
                insights.append(f"ğŸ¯ ì´ìƒì¹˜ê°€ ë§ì€ ë³€ìˆ˜: {', '.join(set(outlier_cols))}")
        
        return insights
    
    def export_analysis_report(self, output_path: str = 'analysis_report.json'):
        """
        ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°
        
        Args:
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'analysis_results': self.analysis_results
        }
        
        try:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"ë¶„ì„ ë¦¬í¬íŠ¸ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            self.logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
