#!/usr/bin/env python3
"""
PostgreSQL ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ëª¨ë“ˆ
ë°ì´í„° ë¬´ê²°ì„±, ì¼ê´€ì„±, ì™„ì „ì„± ë“±ì„ ê²€ì‚¬í•˜ëŠ” ê¸°ëŠ¥ ì œê³µ
"""

from db_connection import PostgreSQLConnection
from typing import Dict, List, Any, Optional
import json
from datetime import datetime

class PostgreSQLDataQualityChecker:
    """PostgreSQL ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.db = PostgreSQLConnection()
    
    def connect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        return self.db.connect()
    
    def disconnect(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ"""
        self.db.disconnect()
    
    def check_null_values(self, table_name: str) -> Dict[str, Any]:
        """NULL ê°’ ê²€ì‚¬"""
        try:
            # í…Œì´ë¸”ì˜ ëª¨ë“  ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ
            columns_query = """
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns
            WHERE table_name = %s AND table_schema = 'public'
            ORDER BY ordinal_position
            """
            columns = self.db.execute_query(columns_query, (table_name,))
            
            if not columns:
                return {'error': 'Table not found or no columns'}
            
            null_analysis = []
            
            for col in columns:
                col_name = col['column_name']
                
                # NULL ê°’ ê°œìˆ˜ ì¡°íšŒ
                null_count_query = f"""
                SELECT 
                    COUNT(*) as total_rows,
                    COUNT("{col_name}") as non_null_count,
                    COUNT(*) - COUNT("{col_name}") as null_count,
                    ROUND(
                        (COUNT(*) - COUNT("{col_name}")) * 100.0 / NULLIF(COUNT(*), 0), 2
                    ) as null_percentage
                FROM "{table_name}"
                """
                
                result = self.db.execute_query(null_count_query)
                
                if result:
                    row = result[0]
                    null_analysis.append({
                        'column_name': col_name,
                        'data_type': col['data_type'],
                        'is_nullable': col['is_nullable'],
                        'total_rows': row['total_rows'],
                        'null_count': row['null_count'],
                        'null_percentage': float(row['null_percentage'] or 0),
                        'quality_score': 100 - float(row['null_percentage'] or 0)
                    })
            
            return {
                'table_name': table_name,
                'analysis_type': 'null_values',
                'columns': null_analysis,
                'summary': {
                    'total_columns': len(null_analysis),
                    'columns_with_nulls': len([c for c in null_analysis if c['null_count'] > 0]),
                    'avg_quality_score': round(sum(c['quality_score'] for c in null_analysis) / len(null_analysis), 2) if null_analysis else 0
                }
            }
            
        except Exception as e:
            return {'error': f'NULL ê°’ ê²€ì‚¬ ì‹¤íŒ¨: {e}'}
    
    def check_duplicate_values(self, table_name: str, columns: Optional[List[str]] = None) -> Dict[str, Any]:
        """ì¤‘ë³µ ê°’ ê²€ì‚¬"""
        try:
            if not columns:
                # ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ì¤‘ë³µ ê²€ì‚¬
                columns_query = """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = %s AND table_schema = 'public'
                ORDER BY ordinal_position
                """
                column_results = self.db.execute_query(columns_query, (table_name,))
                columns = [col['column_name'] for col in column_results]
            
            duplicate_analysis = []
            
            for col_name in columns:
                # ì¤‘ë³µ ê°’ ê²€ì‚¬
                dup_query = f"""
                SELECT 
                    COUNT(*) as total_rows,
                    COUNT(DISTINCT "{col_name}") as unique_values,
                    COUNT(*) - COUNT(DISTINCT "{col_name}") as duplicate_count,
                    ROUND(
                        (COUNT(*) - COUNT(DISTINCT "{col_name}")) * 100.0 / NULLIF(COUNT(*), 0), 2
                    ) as duplicate_percentage
                FROM "{table_name}"
                WHERE "{col_name}" IS NOT NULL
                """
                
                result = self.db.execute_query(dup_query)
                
                if result:
                    row = result[0]
                    
                    # ê°€ì¥ ë¹ˆë²ˆí•œ ì¤‘ë³µê°’ ì¡°íšŒ
                    frequent_query = f"""
                    SELECT "{col_name}" as value, COUNT(*) as frequency
                    FROM "{table_name}"
                    WHERE "{col_name}" IS NOT NULL
                    GROUP BY "{col_name}"
                    HAVING COUNT(*) > 1
                    ORDER BY COUNT(*) DESC
                    LIMIT 5
                    """
                    
                    frequent_dups = self.db.execute_query(frequent_query)
                    
                    duplicate_analysis.append({
                        'column_name': col_name,
                        'total_rows': row['total_rows'],
                        'unique_values': row['unique_values'],
                        'duplicate_count': row['duplicate_count'],
                        'duplicate_percentage': float(row['duplicate_percentage'] or 0),
                        'uniqueness_score': round(100 - float(row['duplicate_percentage'] or 0), 2),
                        'most_frequent_duplicates': frequent_dups[:5] if frequent_dups else []
                    })
            
            return {
                'table_name': table_name,
                'analysis_type': 'duplicate_values',
                'columns': duplicate_analysis,
                'summary': {
                    'total_columns_checked': len(duplicate_analysis),
                    'columns_with_duplicates': len([c for c in duplicate_analysis if c['duplicate_count'] > 0]),
                    'avg_uniqueness_score': round(sum(c['uniqueness_score'] for c in duplicate_analysis) / len(duplicate_analysis), 2) if duplicate_analysis else 0
                }
            }
            
        except Exception as e:
            return {'error': f'ì¤‘ë³µ ê°’ ê²€ì‚¬ ì‹¤íŒ¨: {e}'}
    
    def check_data_consistency(self, table_name: str) -> Dict[str, Any]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬"""
        try:
            consistency_checks = []
            
            # ìˆ«ì ì»¬ëŸ¼ì˜ ì´ìƒì¹˜ ê²€ì‚¬
            numeric_columns_query = """
            SELECT column_name, data_type
            FROM information_schema.columns
            WHERE table_name = %s AND table_schema = 'public'
                AND data_type IN ('integer', 'bigint', 'decimal', 'numeric', 'real', 'double precision')
            """
            
            numeric_columns = self.db.execute_query(numeric_columns_query, (table_name,))
            
            for col in numeric_columns:
                col_name = col['column_name']
                
                # ê¸°ë³¸ í†µê³„ ì¡°íšŒ
                stats_query = f"""
                SELECT 
                    COUNT(*) as total_count,
                    COUNT("{col_name}") as non_null_count,
                    MIN("{col_name}") as min_value,
                    MAX("{col_name}") as max_value,
                    AVG("{col_name}") as avg_value,
                    STDDEV("{col_name}") as std_dev,
                    PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY "{col_name}") as q1,
                    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY "{col_name}") as median,
                    PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY "{col_name}") as q3
                FROM "{table_name}"
                WHERE "{col_name}" IS NOT NULL
                """
                
                stats = self.db.execute_query(stats_query)
                
                if stats and stats[0]['non_null_count'] > 0:
                    stat = stats[0]
                    
                    # IQRì„ ì´ìš©í•œ ì´ìƒì¹˜ ê²€ì‚¬
                    if stat['q1'] is not None and stat['q3'] is not None:
                        iqr = float(stat['q3']) - float(stat['q1'])
                        lower_bound = float(stat['q1']) - 1.5 * iqr
                        upper_bound = float(stat['q3']) + 1.5 * iqr
                        
                        outlier_query = f"""
                        SELECT COUNT(*) as outlier_count
                        FROM "{table_name}"
                        WHERE "{col_name}" IS NOT NULL
                            AND ("{col_name}" < {lower_bound} OR "{col_name}" > {upper_bound})
                        """
                        
                        outlier_result = self.db.execute_query(outlier_query)
                        outlier_count = outlier_result[0]['outlier_count'] if outlier_result else 0
                        outlier_percentage = round(outlier_count * 100.0 / stat['non_null_count'], 2)
                        
                        consistency_checks.append({
                            'column_name': col_name,
                            'data_type': col['data_type'],
                            'total_values': stat['non_null_count'],
                            'min_value': float(stat['min_value']) if stat['min_value'] is not None else None,
                            'max_value': float(stat['max_value']) if stat['max_value'] is not None else None,
                            'avg_value': round(float(stat['avg_value']), 2) if stat['avg_value'] is not None else None,
                            'std_dev': round(float(stat['std_dev']), 2) if stat['std_dev'] is not None else None,
                            'q1': float(stat['q1']) if stat['q1'] is not None else None,
                            'median': float(stat['median']) if stat['median'] is not None else None,
                            'q3': float(stat['q3']) if stat['q3'] is not None else None,
                            'outlier_count': outlier_count,
                            'outlier_percentage': outlier_percentage,
                            'consistency_score': max(0, round(100 - outlier_percentage, 2))
                        })
            
            return {
                'table_name': table_name,
                'analysis_type': 'data_consistency',
                'numeric_columns': consistency_checks,
                'summary': {
                    'total_numeric_columns': len(consistency_checks),
                    'columns_with_outliers': len([c for c in consistency_checks if c['outlier_count'] > 0]),
                    'avg_consistency_score': round(sum(c['consistency_score'] for c in consistency_checks) / len(consistency_checks), 2) if consistency_checks else 0
                }
            }
            
        except Exception as e:
            return {'error': f'ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}'}
    
    def check_referential_integrity(self, table_name: str) -> Dict[str, Any]:
        """ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬"""
        try:
            # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ì¡°íšŒ
            fk_query = """
            SELECT 
                kcu.column_name,
                ccu.table_name AS foreign_table_name,
                ccu.column_name AS foreign_column_name,
                tc.constraint_name
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage kcu
                ON tc.constraint_name = kcu.constraint_name
            JOIN information_schema.constraint_column_usage ccu
                ON ccu.constraint_name = tc.constraint_name
            WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_name = %s
            """
            
            foreign_keys = self.db.execute_query(fk_query, (table_name,))
            
            integrity_checks = []
            
            for fk in foreign_keys:
                # ì°¸ì¡° ë¬´ê²°ì„± ìœ„ë°˜ ê²€ì‚¬
                violation_query = f"""
                SELECT COUNT(*) as violation_count
                FROM "{table_name}" t
                LEFT JOIN "{fk['foreign_table_name']}" ft 
                    ON t."{fk['column_name']}" = ft."{fk['foreign_column_name']}"
                WHERE t."{fk['column_name']}" IS NOT NULL
                    AND ft."{fk['foreign_column_name']}" IS NULL
                """
                
                violation_result = self.db.execute_query(violation_query)
                violation_count = violation_result[0]['violation_count'] if violation_result else 0
                
                # ì´ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
                total_query = f"""
                SELECT COUNT(*) as total_count
                FROM "{table_name}"
                WHERE "{fk['column_name']}" IS NOT NULL
                """
                
                total_result = self.db.execute_query(total_query)
                total_count = total_result[0]['total_count'] if total_result else 0
                
                violation_percentage = round(violation_count * 100.0 / total_count, 2) if total_count > 0 else 0
                
                integrity_checks.append({
                    'constraint_name': fk['constraint_name'],
                    'column_name': fk['column_name'],
                    'foreign_table': fk['foreign_table_name'],
                    'foreign_column': fk['foreign_column_name'],
                    'total_references': total_count,
                    'violation_count': violation_count,
                    'violation_percentage': violation_percentage,
                    'integrity_score': round(100 - violation_percentage, 2)
                })
            
            return {
                'table_name': table_name,
                'analysis_type': 'referential_integrity',
                'foreign_keys': integrity_checks,
                'summary': {
                    'total_foreign_keys': len(integrity_checks),
                    'keys_with_violations': len([c for c in integrity_checks if c['violation_count'] > 0]),
                    'avg_integrity_score': round(sum(c['integrity_score'] for c in integrity_checks) / len(integrity_checks), 2) if integrity_checks else 100
                }
            }
            
        except Exception as e:
            return {'error': f'ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}'}
    
    def comprehensive_quality_check(self, table_name: str) -> Dict[str, Any]:
        """ì¢…í•© ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
        try:
            print(f"\nğŸ” í…Œì´ë¸” '{table_name}' ë°ì´í„° í’ˆì§ˆ ì¢…í•© ê²€ì‚¬ ì‹œì‘...")
            
            results = {
                'table_name': table_name,
                'analysis_date': datetime.now().isoformat(),
                'checks': {}
            }
            
            # 1. NULL ê°’ ê²€ì‚¬
            print("   ğŸ“‹ NULL ê°’ ê²€ì‚¬ ì¤‘...")
            null_check = self.check_null_values(table_name)
            results['checks']['null_values'] = null_check
            
            # 2. ì¤‘ë³µ ê°’ ê²€ì‚¬
            print("   ğŸ”„ ì¤‘ë³µ ê°’ ê²€ì‚¬ ì¤‘...")
            dup_check = self.check_duplicate_values(table_name)
            results['checks']['duplicate_values'] = dup_check
            
            # 3. ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬
            print("   ğŸ“Š ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬ ì¤‘...")
            consistency_check = self.check_data_consistency(table_name)
            results['checks']['data_consistency'] = consistency_check
            
            # 4. ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬
            print("   ğŸ”— ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘...")
            integrity_check = self.check_referential_integrity(table_name)
            results['checks']['referential_integrity'] = integrity_check
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            scores = []
            if 'summary' in null_check:
                scores.append(null_check['summary'].get('avg_quality_score', 0))
            if 'summary' in dup_check:
                scores.append(dup_check['summary'].get('avg_uniqueness_score', 0))
            if 'summary' in consistency_check:
                scores.append(consistency_check['summary'].get('avg_consistency_score', 0))
            if 'summary' in integrity_check:
                scores.append(integrity_check['summary'].get('avg_integrity_score', 0))
            
            overall_score = round(sum(scores) / len(scores), 2) if scores else 0
            
            results['overall_quality_score'] = overall_score
            results['quality_grade'] = self._get_quality_grade(overall_score)
            
            return results
            
        except Exception as e:
            return {'error': f'ì¢…í•© ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}'}
    
    def _get_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 95:
            return "A+ (ìµœìš°ìˆ˜)"
        elif score >= 90:
            return "A (ìš°ìˆ˜)"
        elif score >= 85:
            return "B+ (ì–‘í˜¸)"
        elif score >= 80:
            return "B (ë³´í†µ)"
        elif score >= 70:
            return "C (ê°œì„  í•„ìš”)"
        else:
            return "D (ëŒ€í­ ê°œì„  í•„ìš”)"
    
    def print_quality_report(self, quality_results: Dict[str, Any]):
        """í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        if 'error' in quality_results:
            print(f"âŒ {quality_results['error']}")
            return
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ë³´ê³ ì„œ")
        print(f"{'='*60}")
        print(f"í…Œì´ë¸”: {quality_results.get('table_name', 'N/A')}")
        print(f"ê²€ì‚¬ì¼ì‹œ: {quality_results.get('analysis_date', 'N/A')}")
        print(f"ì¢…í•© ì ìˆ˜: {quality_results.get('overall_quality_score', 0)}/100")
        print(f"í’ˆì§ˆ ë“±ê¸‰: {quality_results.get('quality_grade', 'N/A')}")
        print(f"{'='*60}")
        
        checks = quality_results.get('checks', {})
        
        # NULL ê°’ ê²€ì‚¬ ê²°ê³¼
        if 'null_values' in checks and 'summary' in checks['null_values']:
            null_summary = checks['null_values']['summary']
            print(f"\nğŸ” NULL ê°’ ê²€ì‚¬:")
            print(f"   â€¢ ê²€ì‚¬ ì»¬ëŸ¼ ìˆ˜: {null_summary.get('total_columns', 0)}")
            print(f"   â€¢ NULL í¬í•¨ ì»¬ëŸ¼: {null_summary.get('columns_with_nulls', 0)}")
            print(f"   â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {null_summary.get('avg_quality_score', 0)}/100")
        
        # ì¤‘ë³µ ê°’ ê²€ì‚¬ ê²°ê³¼
        if 'duplicate_values' in checks and 'summary' in checks['duplicate_values']:
            dup_summary = checks['duplicate_values']['summary']
            print(f"\nğŸ”„ ì¤‘ë³µ ê°’ ê²€ì‚¬:")
            print(f"   â€¢ ê²€ì‚¬ ì»¬ëŸ¼ ìˆ˜: {dup_summary.get('total_columns_checked', 0)}")
            print(f"   â€¢ ì¤‘ë³µ í¬í•¨ ì»¬ëŸ¼: {dup_summary.get('columns_with_duplicates', 0)}")
            print(f"   â€¢ í‰ê·  ê³ ìœ ì„± ì ìˆ˜: {dup_summary.get('avg_uniqueness_score', 0)}/100")
        
        # ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬ ê²°ê³¼
        if 'data_consistency' in checks and 'summary' in checks['data_consistency']:
            consistency_summary = checks['data_consistency']['summary']
            print(f"\nğŸ“Š ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬:")
            print(f"   â€¢ ìˆ«ì ì»¬ëŸ¼ ìˆ˜: {consistency_summary.get('total_numeric_columns', 0)}")
            print(f"   â€¢ ì´ìƒì¹˜ í¬í•¨ ì»¬ëŸ¼: {consistency_summary.get('columns_with_outliers', 0)}")
            print(f"   â€¢ í‰ê·  ì¼ê´€ì„± ì ìˆ˜: {consistency_summary.get('avg_consistency_score', 0)}/100")
        
        # ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬ ê²°ê³¼
        if 'referential_integrity' in checks and 'summary' in checks['referential_integrity']:
            integrity_summary = checks['referential_integrity']['summary']
            print(f"\nğŸ”— ì°¸ì¡° ë¬´ê²°ì„± ê²€ì‚¬:")
            print(f"   â€¢ ì™¸ë˜í‚¤ ìˆ˜: {integrity_summary.get('total_foreign_keys', 0)}")
            print(f"   â€¢ ìœ„ë°˜ ë°œê²¬ í‚¤: {integrity_summary.get('keys_with_violations', 0)}")
            print(f"   â€¢ í‰ê·  ë¬´ê²°ì„± ì ìˆ˜: {integrity_summary.get('avg_integrity_score', 0)}/100")

def main():
    """í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ í•¨ìˆ˜"""
    checker = PostgreSQLDataQualityChecker()
    
    if not checker.connect():
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨!")
        return
    
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” ì¡°íšŒ
        tables_query = """
        SELECT tablename, pg_size_pretty(pg_total_relation_size(quote_ident(tablename))) as size
        FROM pg_tables
        WHERE schemaname = 'public'
        ORDER BY pg_total_relation_size(quote_ident(tablename)) DESC
        LIMIT 5
        """
        
        tables = checker.db.execute_query(tables_query)
        
        if tables:
            print("ğŸ“‹ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ê°€ëŠ¥í•œ í…Œì´ë¸” (ìƒìœ„ 5ê°œ):")
            for i, table in enumerate(tables, 1):
                print(f"   {i}. {table['tablename']} ({table['size']})")
                
            print("\nğŸ’¡ ì‚¬ìš©ë²•:")
            print("   from data_quality_checker import PostgreSQLDataQualityChecker")
            print("   checker = PostgreSQLDataQualityChecker()")
            print("   checker.connect()")
            print("   results = checker.comprehensive_quality_check('í…Œì´ë¸”ëª…')")
            print("   checker.print_quality_report(results)")
    
    finally:
        checker.disconnect()

if __name__ == "__main__":
    main()
